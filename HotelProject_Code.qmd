---
title: "Hotel Booking - Data Cleaning"
subtitle: "Group: High5!"
author:
  - name: Luke Richardson
  - name: Marco Molnar 
  - name: Marina Zhai
  - name: Omar Yousef
  - name: Taeha Oh
date: "December 4, 2025"
format:
  html:
    embed-resources: true
    self-contained: true
    self-contained-math: true
    toc: true
    toc-location: left
    toc-depth: 6
    theme: cosmo
    code-fold: true
    code-tools: true 
highlight-style: dracula
---

# R Setup

```{r}
library(dplyr)
library(rpart)
library(rpart.plot)
library(tidyr)
library(skimr)
library(forcats)
library(ggplot2)
library(caret)
library(glmnet)
library(C50)
library(neuralnet)
library(class)
library(randomForest)
library(kernlab)
library(pROC)
library(ipred)
library(ggnewscale) 
```

# Step 0: Our Setup

### Purpose of the Project: Why are We doing this Project?

The goal of this project is to develop a predictive model that **estimates the probability that a newly received hotel reservation will be cancelled or not honored by the guest.** Cancellations and no-shows represent a major source of uncertainty for hotels, as they **directly affect occupancy, revenue, operational planning, and customer experience.** Traditional approaches to handling this uncertainty—such as fixed overbooking quotas—are often based on intuition rather than data-driven insights and can either lead to unused capacity or to costly overbooking situations.

By building a **machine learning model that forecasts cancellation likelihood**, we aim to support hotels in making **more informed decisions about overbooking, capacity planning, and pricing**. A more accurate prediction of booking reliability allows hotels to sell the optimal number of rooms, reduce the risk of walking guests to other properties, stabilize revenue, and improve resource allocation in areas such as housekeeping and front-office operations. Ultimately, this enhances both financial performance and service quality.

### Baseline Scenario

If there is no algorithm predicting if a reservation will be canceled or not, we call this the baseline scenario. Let's have a look at the distribution of the dependent variable. We see that 37% of all reservations are canceled. We take this fraction as **probabilty that a reservation will be canceld (p)** and its counter probabilty as the **probability that a guest will end up staying in the hotel (1-p)**.

```{r}
##### Baseline Scenario
data <- read.csv("hotel_booking.csv")
data_analysis <- data %>%
  count(is_canceled) %>%
  mutate(percent = round(100 * n / sum(n), 1))

ggplot(data_analysis, aes(x = factor(is_canceled), y = n, fill = factor(is_canceled))) +
  geom_col(color = "black") +
  geom_text(aes(label = paste0(n, " (", percent, "%)")),
            vjust = -0.5, size = 5) +
  labs(title = "Distribution of Dependent Variable",
       x = "Outcome (is_canceled)", y = "Count", fill = "is_canceled") +
  expand_limits(y = max(data_analysis$n) * 1.1) +
  theme_minimal()

```

### Setting up a Profit Function

To evaluate the economic impact of our prediction model, we assign a monetary value to each outcome of the confusion matrix. Our goal is to express how much profit (or loss) the hotel generates depending on whether a booking is correctly or incorrectly predicted as a cancellation. The following assumptions are intentionally simplified but realistic for hotel operations.

#### Basic Financial Assumptions

##### Average Daily Rate (ADR)**
Let's first look at the Average Daily Rate (ADR) of all customer in the dataset:

```{r}
summary(data$adr)
```

So the ADR is \$100 per night.

##### Variable Operating Costs
Costs that occur only if a room is occupied (cleaning, utilities, amenities):

- We assume \$30 variable costs per night

So the profit from a correctly occupied room is therefore:

- Profit per occupied room: \$100 − \$30 = \$70

**This \$70 represents our baseline profit for a correctly predicted stay.**

##### Cost of walking a guest (overbooking compensation)

When the hotel is overbooked, one guest may need to be relocated (“walked”) to another hotel. This typically includes:

- Paying the alternative hotel: \$100
- Transport compensation (e.g., taxi): \$20
- Additional compensation (voucher or goodwill gesture): \$20
- So total walk cost: \$100 + \$20 + \$20 = \$140


#### Economic Value of Each Confusion Matrix Outcome

##### True Negative (TN) – correctly predicted non-cancellation

The model predicts that the guest will show up, and they do.

- Revenue: \$100
- Variable cost: \$30
- Profit(TN) = \$100 - \$30 = \$70

This is the ideal and most common case.

##### True Positive (TP) – correctly predicted cancellation

The model predicts that the guest will cancel, and the guest indeed cancels.
Because the hotel anticipates the cancellation, it can safely overbook and replace the cancelled booking with another guest. But that the next guest will end up taking his reservation is not guaranteed. Therefore we take advantage of the calculated probabilities above:

- Profit(TP) = \$100 - \$30 = (1-p) \cdot \$70 + p \cdot -\$70
- With p=0.37: Expected Profit of one TP = (1-0.37) \cdot \$70 + 0.37 \cdot -\$70

##### False Negative (FN) – missed cancellation

The model predicts that the guest will arrive, but the guest cancels. The hotel does not overbook and the room remains empty.

- Revenue: \$0
- Variable cost: \$0

But the **economic loss** comes from the missed opportunity to sell this room:

- Opportunity cost(FN) = -Profit(TN) = -Profit(TP) = \$-70

##### False Positive (FP) – wrongly predicted cancellation → overbooking

The model predicts cancellation, but the guest actually arrives. Because the hotel overbooks based on this prediction, it now has more guests than rooms, forcing it to walk one guest.

Revenue:
- Revenue from the guest who stays in the hotel: \$100

Costs:
- Variable cost for the occupied room: -\$30
- Cost for external hotel: -\$100
- Taxi reimbursement: -\$20
- Compensation voucher: -\$20

Profit(FP) = \$100 - \$30 - \$100 - \$20 - \$20 = -\$70

Relative to the ideal profit of \$70, this represents a total economic loss of:

- Total cost(FP) = -\$70 - \$70 = -\$140

**This is significantly higher than the cost of an FN because a FP not only eliminates the expected profit but also introduces real financial penalties.**

##### Resulting Profit Function

A profit function based on these values is:

$$
Profit = 70 \cdot TN + 18.20 \cdot TP - 140 \cdot FP - 70 \cdot FN
$$

```{r}
gain_function <- function(cm) {
  return(70 * cm$table[1,1] + 18.2 * cm$table[2, 2] + cm$table[2, 1] * -140 + cm$table[1, 2] * -70)}

results <- data.frame(model = integer(),
                      threshold = numeric(),
                      accuracy = numeric(),
                      sensitivity = numeric(),
                      specificity = numeric(),
                      kappa = numeric(),
                      gain = numeric(),
                      stringsAsFactors = FALSE)
lr_results <- results
knn_results <- results
ann_results <- results
svm_results <- results
dt_results <- results
rf_results <- results

```

::: {.callout-tip title="Hotel Overbooking Problem"}
To quantify the economic impact of model predictions, we assign a monetary value to each cell of the confusion matrix. We assume an average daily rate (ADR) of 100 € and variable operating costs of 30 € per occupied room, resulting in a profit of 70 € for a correctly occupied room. A true negative (correctly predicted non-cancellation) and a true positive (correctly predicted cancellation that can be compensated by overbooking) therefore both generate a profit of 70 € per booking.

In contrast, a false negative (the model predicts “not cancelled”, but the booking is cancelled) leaves the room empty. In this case, the hotel generates no revenue and incurs no variable costs, resulting in a profit of 0 €. Relative to the ideal profit of 70 €, a false negative corresponds to an opportunity cost of 70 €.

A false positive (the model predicts “cancelled”, but the guest actually arrives) is economically more severe. In an overbooked situation, the hotel must “walk” a guest to another property, which we model as a fixed cost of 140 € (covering the external room night, transportation, and compensation). Taking into account the profit loss relative to a correctly handled booking, the economic loss associated with a false positive is substantially higher than that of a false negative. This asymmetry justifies assigning a higher cost to false positives in the profit function and focusing on high specificity to keep the number of false positives low.
:::


# Step 1 & 2: Load and Clean the Data

```{r}
skim(data)
summary(data)
```

### Artificial Variables

The variables `name`, `email`, `phone-number` and `credit-card` are not real variables because they were imputed to protect personel data. For this reason, we will not consider these artificial variables as features, because we don't know ecactly how the imputation worked. It is only stated that these variables **... have been artificially created using a python and filled into the dataset**. 

```{r}
data <- data %>%
  select(-name, -email, -phone.number,-credit_card)

```


### Handle N/As

We can directly observe that for `agent` and especially for `company` we have a lot of missing values. The completion rate of the latter is just 5.6%. So the naive approach would be to just A) delete all observations where we have a missing value or B) ignore the two independent variables as predictors. But we believe these variables still have some interesting information and therefore we proceed as follows:

### Company

To handle the predictor variable `company`, which contains more than 80% missing values and a large number of rare categories, we grouped the companies into the five most common categories and combined all remaining companies into a single “Other” category. We then created dummy variables for these groups. This approach reduces noise from many low-frequency categories, prevents overfitting, and ensures that the model focuses on the most informative company categories while still retaining a meaningful comparison group.

- We **add somthing here**?? 
-


```{r}
# Count non-NA companies and pick top 5
top5 <- data %>%
  filter(!is.na(company)) %>%
  count(company, name = "n") %>%
  arrange(desc(n)) %>%
  slice_head(n = 5)

# Create a named *character* vector: company_value → "Company_TopX"
top5_names <- as.character(top5$company)     # convert to character
names(top5_names) <- paste0("Company_Top", seq_along(top5_names))

# Recode companies into Company_TopX or Company_Others
data <- data %>%
  mutate(
    company_group = case_when(
      company %in% top5$company ~ names(top5_names)[match(as.character(company), top5_names)],
      TRUE ~ "Company_Others"   # includes NA automatically
    )
  )

data$company_group <- as.factor(data$company_group)
summary(data$company_group)
data <- data %>%
  select(-company)


```

### Agent
- We follow the same logic we used for `company`, but now for `agent`

```{r}
# 1. Count non-NA agents and pick top 5
top5_agent <- data %>%
  filter(!is.na(agent)) %>%
  count(agent, name = "n") %>%
  arrange(desc(n)) %>%
  slice_head(n = 5)


# 2. Create a named *character* vector: agent_value -> "Agent_TopX"
top5_agent_vals <- as.character(top5_agent$agent)
names(top5_agent_vals) <- paste0("Agent_Top", seq_along(top5_agent_vals))

# 3. Recode agents into Agent_TopX or Agent_Others
data <- data %>%
  mutate(
    agent_group = case_when(
      as.character(agent) %in% top5_agent_vals ~
        names(top5_agent_vals)[match(as.character(agent), top5_agent_vals)],
      TRUE ~ "Agent_Others"   # includes NA automatically
    ),
    agent_group = factor(agent_group)
  ) %>%
  select(-agent)   # drop original numeric agent variable if you don’t need it anymore

# Quick check
summary(data$agent_group)

```

### Children

```{r}
data <- data %>%
  filter(!is.na(children))
```


### Convert Character to Date

We also notice that `reservation_status_data` is encoded as a character, but it is actually a date. 

```{r}
data$reservation_status_date <- as.Date(data$reservation_status_date)
```

### Only one level factor for Room Type and Distribution Channel

** random blank space? **
 


### Convert Characters to Factors

```{r}
data <- data %>%
  mutate(
    hotel                   = as.factor(hotel),
    arrival_date_month      = as.factor(arrival_date_month),
    meal                    = as.factor(meal),
    country                 = as.factor(country),
    market_segment          = as.factor(market_segment),
    distribution_channel    = as.factor(distribution_channel),
    reserved_room_type      = as.factor(reserved_room_type),
    assigned_room_type      = as.factor(assigned_room_type),
    deposit_type            = as.factor(deposit_type),
    customer_type           = as.factor(customer_type),
    reservation_status      = as.factor(reservation_status)
  )
summary(data)
```

```{r}
data <- subset(data, assigned_room_type != "L")
data <- subset(data, assigned_room_type != "P")
data <- subset(data, distribution_channel != "Undefined")
```


### Data Leakage Problem

As we converted `reservation_status` to factors and have a look at the `str()` report above, we see that we have three different levels. Let's have a closer look at the variable `reservation_status`:

```{r}
summary(data$reservation_status)
```

We see that this variable contains the information if a customer has cancelled his booking or not. This seems weird because this information should capture **only our dependent variable**. 

```{r}
data <- data %>%
  mutate(
    reservation_status = factor(reservation_status),
    # Lump "Canceled" + "No-Show" into one level
    reservation_status_lumped = fct_collapse(
      reservation_status,
      "Canceled_or_NoShow" = c("Canceled", "No-Show"),
      "Check-Out"          = "Check-Out"
    )
  )
data <- data %>%
  mutate(
    canceled_from_status = if_else(
      reservation_status_lumped == "Canceled_or_NoShow",
      1L,  # canceled
      0L   # not canceled (e.g., "Check-Out")
    )
  )
data <- data %>%
  mutate(
    same_info = (is_canceled == canceled_from_status)
  )

# Proportion of rows where they agree
mean(data$same_info)

# Cross-tabulation for more detail
table(is_canceled = data$is_canceled,
      status_binary = data$canceled_from_status)

```

To examine whether the two variables `is_canceled` (our dependent variable) and `reservation_status` contain the same underlying information, we first recoded reservation_status into a binary factor called `status_binary`. In this step, the levels "Canceled" and "No-Show" were grouped together as 1, while "Check-Out" was coded as 0. This allowed the two variables to be placed on the same numerical scale (0/1). We then created a cross-tabulation comparing is_canceled with status_binary. The resulting table shows a perfect match: every observation where is_canceled = 1 also has status_binary = 1, and every case where is_canceled = 0 corresponds to status_binary = 0. **This confirms that the factor variable reservation_status is providing the exact same information as is_canceled, meaning reservation_status is redundant and can be removed from further modeling to avoid information leakage**.

```{r}
data <- data %>%
  select(-reservation_status, -reservation_status_lumped, -canceled_from_status, -same_info)
```

### Country

To reduce the dimensionality of the categorical variable country, we grouped countries according to their contribution to the total frequency in the dataset. First, we ranked all countries by how often they appear. We then identified the smallest set of countries whose cumulative frequency accounts for approximately 80% of all observations. These high-frequency countries were kept as individual factor levels, because they contain most of the information. All remaining low-frequency countries—together with any missing values—were pooled into a single category named “Country_Other”. This approach retains the most informative variation in the data while preventing excessive sparsity, improving model stability and interpretability. **Using this approach, we reduced the dimensionality from 178 levels to just 8 levels and still kept the relevant information.**

```{r}
# Step 1: Count countries and compute cumulative percentage
country_freq <- data %>%
  count(country, name = "n") %>%
  arrange(desc(n)) %>%
  mutate(
    pct = n / sum(n),
    cum_pct = cumsum(pct)
  )

# Step 2: Select countries contributing to ~80%
top_countries <- country_freq %>%
  filter(cum_pct <= 0.80) %>%
  pull(country)

# Step 3: Recode into Top countries vs Other
data <- data %>%
  mutate(
    country_group = case_when(
      country %in% top_countries ~ as.character(country),
      TRUE ~ "Country_Other"
    ),
    country_group = as.factor(country_group)
  )

# Optional: remove original variable
# data <- data %>% select(-country)

# Check final distribution
summary(data$country_group)
data <- data %>% select(-country)
str(data)

```

### Change Name of Dependent Variable

For simplicity, we change the name of the dependent variable to `y`.

```{r}
data$y <- data$is_canceled
data <- data %>%
  select(-is_canceled)
```

```{r}
set.seed(12345)
data <- data %>%
  sample_frac(0.2)
str(data)
```

### Final Datasets for Different Algorithms

#### Logistic Regression

::: {.callout-important title="Checklist for Logistic Regression"}
-   Dependent Variable can be numeric (0/1)
-   Min-Max scaling **is not** necessary
-   Dummy Variables **is not** necessary
:::

```{r}
data_log <- data
```


#### ANN & KNN

::: {.callout-important title="Checklist for ANN and KNN"}
-   Neural networks are sensitive to variable ranges.
-   Dependent Variable can be numeric (0/1)
-   Min-Max scaling **is** necessary
-   Dummy Variables **is** necessary
:::

We first drop the dependent variable, then one-hot encoding all the features to create dummy variables for all categorical variables. In a next step, we use min-max scaling and add back the dependent variable:

```{r}
data_ann <- data

data_ann_no_y <- data_ann %>% select(-y) # drop the dependent variable
data_ann <- as.data.frame(model.matrix(~ . -1, data = data_ann_no_y))

# scale variables to ensure each variable has minimal value of 0 and maximum value of 1
minmax <- function(x){
  (x-min(x))/(max(x)-min(x))
}

data_ann <- as.data.frame(lapply(data_ann, minmax))
data_ann$y <- data$y
data_ann <- data_ann %>%
  select(-distribution_channelUndefined, -reserved_room_typeL, -reserved_room_typeP, -assigned_room_typeL, -assigned_room_typeP)

data_knn <- data_ann
```

#### Supported Vector Machines

::: {.callout-important title="Checklist for SVM"}
-   Dependent Variable **must be a two level factor**
-   Min-Max scaling **is** necessary
-   Dummy Variables **is** necessary
:::

SVM requires the same type of preprocessed feature data as ANN and KNN — fully numeric, dummy-coded for nominal variables, and standardised — because all three methods depend on distance or gradient-based optimisation. The only difference is that SVM needs the dependent variable to be a binary factor, whereas ANN and KNN use a numeric 0/1 target.

```{r}
data_svm <- data_ann

# Dependent variable as factor
data_svm$y <- as.factor(data_svm$y)
```

#### Decision Trees & Random Forest

::: {.callout-important title="Checklist for Decision Trees & Random Forest"}
-   Dependent Variable **should be a two level factor**
-   Min-Max scaling **is not** necessary
-   Dummy Variables **is not** necessary
:::

```{r}
data_dt <- data

data_dt$y <- as.factor(data_dt$y)
data_rf <- data_dt
```

### Export Cleaned Datasets

```{r}
#write.csv(data_log,"data_log_small.csv")
#write.csv(data_ann,"data_ann_small.csv")
#write.csv(data_knn,"data_knn_small.csv")
#write.csv(data_svm,"data_svm_small.csv")
#write.csv(data_dt,"data_dt_small.csv")
#write.csv(data_rf,"data_rf_small.csv")
```


## Doing some EDA! 

```{r}
ggplot(data, aes(x = lead_time, fill = as.factor(y))) +
  geom_histogram(bins = 40, color = "white") +
  scale_fill_manual(values = c("#70AD47", "#C00000")) +
  labs(title = "Lead Time Distribution by Cancellation Status",
       x = "Lead Time (days)", y = "Count")

```

Interpretation: Reservations with longer lead times show a substantially higher cancellation rate. Short-lead reservations (0–30 days) rarely cancel, but far-future bookings show a clear surge in cancellations. This suggests that guests with more time before arrival are more likely to change plans, making lead_time one of the strongest early predictors of cancellation behavior. Operationally, long-horizon bookings should be weighted more heavily in overbooking decisions.


```{r}
ggplot(data, aes(x = deposit_type, fill = as.factor(y))) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("#70AD47", "#C00000")) +
  labs(title = "Cancellation Rate by Deposit Type",
       y = "Proportion")
```


Interpretation: Cancellation behavior varies dramatically by deposit policy. Non-refundable bookings almost never cancel, while refundable and no-deposit bookings show much higher cancellation rates. This validates that cancellation incentives matter: when customers bear financial risk, cancellations collapse. Deposit_type is therefore a high-impact categorical predictor and also a lever hotels can adjust to reduce financial losses.


```{r}
ggplot(data, aes(x = market_segment, fill = as.factor(y))) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("#70AD47", "#C00000")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Cancellation Rate by Market Segment",
       y = "Proportion")

```

## Booking behavior differs strongly across customer segments. Online TA and Groups have the highest cancellation proportions, consistent with more price-sensitive or bulk-booking customers. Corporate, Complementary, and Direct segments show far lower cancellation rates. This helps the model identify high-risk channels and suggests that hotels should tailor overbooking buffers and communication strategies by segment.


# Step 3: Split the Data

In a next step, we split our data set randomly for training and testing purposes. We use a 50% train and 50% test split, ensuring that we have enough data for the stacked model in the second step.

```{r}
set.seed(12345) # set seed for reputability

trainprop <- 0.50 # for first split
train_rows <- sample(1:nrow(data_log), trainprop*nrow(data_log))

# LR
train_first_log <- data_log[train_rows,] # create train data set for first models
test_first_log <- data_log[-train_rows,] # create test data set for first models

# ANN
train_first_ann <- data_ann[train_rows,]
test_first_ann <- data_ann[-train_rows,]

# KNN
train_first_knn <- data_knn[train_rows,]
test_first_knn <- data_knn[-train_rows,]

# SVM
train_first_svm <- data_svm[train_rows,]
test_first_svm <- data_svm[-train_rows,]

# Decision Tree
train_first_dt <- data_dt[train_rows,]
test_first_dt <- data_dt[-train_rows,]

# Random Forest
train_first_rf <- data_rf[train_rows,]
test_first_rf <- data_rf[-train_rows,]
```

# Step 4, 5 & 6: Build Models, Make Predictions and Evaluate

### Logistic Regression (LR)

```{r}
lr_model_base <- glm(y ~ ., data = train_first_log, family = "binomial")

lr_prob_base <- predict(
  lr_model_base,
  newdata = test_first_log,
  type = "response"
)

```

Now we can create a confusion matrix, using a standard threshold of 0.5 - 
```{r}
# Confusion Matrix
lr_pred_base <- ifelse(lr_prob_base >= 0.99999999, 1, 0)

cm_lr_base <- confusionMatrix(
  as.factor(lr_pred_base),
  factor(test_first_log$y, levels = c(0, 1)),
  positive = "1"
)

cm_lr_base
```

```{r}
summary_table <- data.frame(
  model       = character(),
  accuracy    = numeric(),
  kappa       = numeric(),
  sensitivity = numeric(),
  specificity = numeric(),
  profit = numeric(),
  stringsAsFactors = FALSE
)

# Create a summary table
baseline_logistic_regression <- data.frame(
  model       = "Logistic Regression",
  accuracy    = cm_lr_base$overall["Accuracy"],
  kappa       = cm_lr_base$overall["Kappa"],
  sensitivity = cm_lr_base$byClass["Sensitivity"],
  specificity = cm_lr_base$byClass["Specificity"],
  profit = gain_function(cm_lr_base)/(cm_lr_base$table[1,1]+cm_lr_base$table[1,2]+cm_lr_base$table[2,1]+cm_lr_base$table[2,2])
)


summary_table <- rbind(summary_table, baseline_logistic_regression)

summary_long <- summary_table %>%
  pivot_longer(
    cols      = -model,
    names_to  = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = factor(metric,
                    levels = c("accuracy", "kappa",
                               "sensitivity", "specificity",
                               "profit")),         # profit ganz rechts
    model  = factor(model, levels = rev(unique(model)))
  )

# 2) Aufteilen in „blaue Metriken“ und „grünen Profit“
df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

# 3) Heatmap mit zwei Farbskalen
ggplot() +
  # Blaue Kacheln für Accuracy/Kappa/Sens/Spec
  geom_tile(data = df_metrics,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_metrics,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Performance",
    low  = "#D8E6F3",
    high = "#084594"
  ) +

  ggnewscale::new_scale_fill() +   # neue Fill-Skala starten

  # Grüne Kachel für Profit
  geom_tile(data = df_profit,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_profit,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Profit",
    low  = "#e5f5e0",
    high = "#238b45"
  ) +

  labs(
    title = "Model Performance Comparison",
    x     = "Metric",
    y     = "Model"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

Interpretation: The logistic regression model performs extremely well on the test set, with 99.72% accuracy, sensitivity of 0.995, and specificity of 0.998. This means the model correctly identifies both cancellations and non-cancellations with very few mistakes, which is important for reducing costly overbooking errors. The expected profit per booking is approximately 50.46 under our defined cost structure, showing that the model remains effective even when accounting for the asymmetric cost of false negatives. Logistic regression also highlights key predictors—lead time, deposit type, and market segment—making it a strong and interpretable baseline for understanding cancellation behavior.

```{r}
# 1) Long-Format bauen UND dabei die Reihenfolge der Metriken festlegen
summary_long <- summary_table %>%
  pivot_longer(
    cols      = -model,
    names_to  = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = factor(metric,
                    levels = c("accuracy", "kappa",
                               "sensitivity", "specificity",
                               "profit")),         # profit ganz rechts
    model  = factor(model, levels = rev(unique(model)))
  )

# 2) Aufteilen in „blaue Metriken“ und „grünen Profit“
df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

# 3) Heatmap mit zwei Farbskalen
ggplot() +
  # Blaue Kacheln für Accuracy/Kappa/Sens/Spec
  geom_tile(data = df_metrics,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_metrics,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Performance",
    low  = "#D8E6F3",
    high = "#084594"
  ) +

  ggnewscale::new_scale_fill() +   # neue Fill-Skala starten

  # Grüne Kachel für Profit
  geom_tile(data = df_profit,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_profit,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Profit",
    low  = "#e5f5e0",
    high = "#238b45"
  ) +

  labs(
    title = "Model Performance Comparison",
    x     = "Metric",
    y     = "Model"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```
** duplicate?  **

```{r}
lr_model_base <- glm(y ~ ., data = train_first_log, family = binomial)
#lr_model_advanced <- glm(y ~ . + studytime:freetime + goout:freetime + studytime:goout, data = train_first_log, family = binomial)
lr_model_step <- step(glm(y ~ ., data = train_first_log, family = binomial), trace = 0)

## new line to avoid error added by marina: 
lr_results_1 <- data.frame()

thresholds <- seq(0, 1, by = 0.5)
for (A in c("base", "step")) {
  for (t in thresholds) {
    assign(paste0("lr_prob_", A), predict(get(paste0("lr_model_", A)), newdata = test_first_log, type = "response"))
    assign(paste0("lr_pred_", A), ifelse(get(paste0("lr_prob_", A)) >= t, "1", "0"))
    assign(paste0("lr_cm_", A), confusionMatrix(as.factor(get(paste0("lr_pred_", A))), as.factor(test_first_log$y), positive = "1"))
    cm <- confusionMatrix(as.factor(get(paste0("lr_pred_", A))), as.factor(test_first_log$y), positive = "1")
    print(cm)
    assign(paste0("lr_gain_", A), gain_function(cm))
    assign(paste0("lr_kappa_", A), as.numeric(cm$overall["Kappa"]))
    assign(paste0("lr_accuracy_", A), as.numeric(cm$overall["Accuracy"]))
    assign(paste0("lr_sensitivity_", A), as.numeric(cm$byClass["Sensitivity"]))
    assign(paste0("lr_specificity_", A), as.numeric(cm$byClass["Specificity"]))
    lr_results_1 <- rbind(lr_results_1, data.frame(model = A, threshold = t,
                                                   kappa = get(paste0("lr_kappa_", A)),
                                                   gain = get(paste0("lr_gain_", A)),
                                                   accuracy = get(paste0("lr_accuracy_", A)),
                                                   sensitivity = get(paste0("lr_sensitivity_", A)),
                                                   specificity = get(paste0("lr_specificity_", A)),
                                                   stringsAsFactors = FALSE, row.names = NULL))
  }
  assign(paste0("lr_standard_", A), head(lr_results_1[lr_results_1$model == A & lr_results_1$threshold == 0.5, ], 1))
  assign(paste0("lr_maxgain_", A), lr_results_1[lr_results_1$model == A, ][which.max(lr_results_1$gain[lr_results_1$model == A]), ])
  assign(paste0("lr_maxkappa_", A), lr_results_1[lr_results_1$model == A, ][which.max(lr_results_1$kappa[lr_results_1$model == A]), ])
}
```

### KNN

#### Basic KNN

```{r}
library(class)
train_first_knn_x <- train_first_knn[,-83]
test_first_knn_x <- test_first_knn[,-83]
train_first_knn_y <- train_first_knn[,83]
test_first_knn_y <- test_first_knn[,83]

model_knn_a <- knn(train = train_first_knn_x, test = test_first_knn_x, cl = train_first_knn_y, k = 20, prob = TRUE)
y_true <- test_first_knn_y
y_pred <- model_knn_a

model_knn_a_conf <- confusionMatrix(y_pred, as.factor(y_true), positive = "1")
model_knn_a_conf

model_knn_a_prob <- ifelse(model_knn_a == "1", attr(model_knn_a, "prob"), 1 - attr(model_knn_a, "prob"))

```

```{r}
knn_baseline_row <- data.frame(
  model       = "Basic KNN",
  accuracy    = model_knn_a_conf$overall["Accuracy"],
  kappa       = model_knn_a_conf$overall["Kappa"],
  sensitivity = model_knn_a_conf$byClass["Sensitivity"],
  specificity = model_knn_a_conf$byClass["Specificity"],
  profit = gain_function(model_knn_a_conf)/(model_knn_a_conf$table[1,1]+model_knn_a_conf$table[1,2]+model_knn_a_conf$table[2,1]+model_knn_a_conf$table[2,2])
)

summary_table <- rbind(summary_table, knn_baseline_row)

summary_long <- summary_table %>%
  pivot_longer(
    cols      = -model,
    names_to  = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = factor(metric,
                    levels = c("accuracy", "kappa",
                               "sensitivity", "specificity",
                               "profit")),         # profit ganz rechts
    model  = factor(model, levels = rev(unique(model)))
  )

# 2) Aufteilen in „blaue Metriken“ und „grünen Profit“
df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

# 3) Heatmap mit zwei Farbskalen
ggplot() +
  # Blaue Kacheln für Accuracy/Kappa/Sens/Spec
  geom_tile(data = df_metrics,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_metrics,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Performance",
    low  = "#D8E6F3",
    high = "#084594"
  ) +

  ggnewscale::new_scale_fill() +   # neue Fill-Skala starten

  # Grüne Kachel für Profit
  geom_tile(data = df_profit,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_profit,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Profit",
    low  = "#e5f5e0",
    high = "#238b45"
  ) +

  labs(
    title = "Model Performance Comparison",
    x     = "Metric",
    y     = "Model"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )


```


Interpretation: 
The basic KNN model performs much worse than logistic regression. Its accuracy is 77.9%, and the Kappa score (0.515) shows only moderate agreement with the true labels. KNN especially struggles to detect cancellations: sensitivity is 0.651, so it misses about one-third of all real cancellations.
Specificity is higher (0.854), meaning it is better at identifying non-cancellations than cancellations, which a problem for overbooking decisions, where missed cancellations are costly. This weakness shows up in the profit metric as well: KNN produces only ~20.08 units of profit, less than half of logistic regression.

Overall, KNN does not capture cancellation behavior well enough to be useful for hotel decisions.


#### Advanced KNN

```{r}
K <- seq (1, 200, by = 10)

for (t in K) {
  knn_model_a <- knn(train = train_first_knn_x, test = test_first_knn_x, cl = train_first_knn_y, k = t, prob = TRUE)
  knn_y_true <- test_first_knn_y
  knn_y_pred <- knn_model_a
  cm <- confusionMatrix(as.factor(knn_y_pred), as.factor(knn_y_true), positive = "1")
  knn_gain <- gain_function(cm)
  knn_kappa_a <- as.numeric(cm$overall["Kappa"])
  knn_accuracy_a <- as.numeric(cm$overall["Accuracy"])
  knn_sensitivity_a <- as.numeric(cm$byClass["Sensitivity"])
  knn_specificity_a <- as.numeric(cm$byClass["Specificity"])
  knn_results <- rbind(knn_results, data.frame(model = "knn_a", threshold = t,
                                                   kappa = knn_kappa_a,
                                                   gain = knn_gain,
                                                   accuracy = knn_accuracy_a,
                                                   sensitivity = knn_sensitivity_a,
                                                   specificity = knn_specificity_a,
                                                   stringsAsFactors = FALSE, row.names = NULL))
  }
knn_maxgain_results <- knn_results[which.max(knn_results$gain), ]
knn_maxkappa_results <- knn_results[which.max(knn_results$kappa), ]
knn_maxgain <- max(knn_results$gain)

knn_prob <- ifelse(knn_model_a == "1", attr(knn_model_a, "prob"), 1 - attr(knn_model_a, "prob"))
```

```{r}
best_k <- knn_maxkappa_results$threshold
cat("Best k by Kappa:", best_k, "\n")

best_knn_pred <- knn(
  train = train_first_knn_x,
  test  = test_first_knn_x,
  cl    = train_first_knn_y,
  k     = best_k,
  prob  = TRUE
)

cm_best_k_kappa <- confusionMatrix(
  as.factor(best_knn_pred),
  as.factor(test_first_knn_y),
  positive = "1"
)

cm_best_k_kappa
```

```{r}
best_k_profit <- knn_maxgain_results$threshold
cat("Best k by Profit:", best_k_profit, "\n")

best_knn_profit_pred <- knn(
  train = train_first_knn_x,
  test  = test_first_knn_x,
  cl    = train_first_knn_y,
  k     = best_k_profit,
  prob  = TRUE
)

cm_best_k_profit <- confusionMatrix(
  as.factor(best_knn_profit_pred),
  as.factor(test_first_knn_y),
  positive = "1"
)

cm_best_k_profit
```

```{r}
ggplot(knn_results, aes(x = threshold, y = kappa)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = knn_maxkappa_results$threshold, linetype = "dashed") +
  labs(title = "KNN performance by k measured with Kappa", x = "K", y = "Kappa") +
  theme_minimal()

ggplot(knn_results, aes(x = threshold, y = gain)) +
  geom_line() +
  geom_point() +
  geom_vline(xintercept = knn_maxgain_results$threshold, linetype = "dashed") +
  scale_x_continuous(limits = c(0, 210)) +
  labs(title = "KNN performance by k measured by Profit", x = "K", y = "Profit") +
  theme_minimal()


```

```{r}
knn_best_k_kappa <- data.frame(
  model       = "KNN best k for kappa",
  accuracy    = cm_best_k_kappa$overall["Accuracy"],
  kappa       = cm_best_k_kappa$overall["Kappa"],
  sensitivity = cm_best_k_kappa$byClass["Sensitivity"],
  specificity = cm_best_k_kappa$byClass["Specificity"],
  profit = gain_function(cm_best_k_kappa)/(cm_best_k_kappa$table[1,1]+cm_best_k_kappa$table[1,2]+cm_best_k_kappa$table[2,1]+cm_best_k_kappa$table[2,2])
)

# An bestehende summary_table anhängen
summary_table <- rbind(summary_table, knn_best_k_kappa)

summary_long <- summary_table %>%
  pivot_longer(
    cols      = -model,
    names_to  = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = factor(metric,
                    levels = c("accuracy", "kappa",
                               "sensitivity", "specificity",
                               "profit")),         # profit ganz rechts
    model  = factor(model, levels = rev(unique(model)))
  )

# 2) Aufteilen in „blaue Metriken“ und „grünen Profit“
df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

# 3) Heatmap mit zwei Farbskalen
ggplot() +
  # Blaue Kacheln für Accuracy/Kappa/Sens/Spec
  geom_tile(data = df_metrics,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_metrics,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Performance",
    low  = "#D8E6F3",
    high = "#084594"
  ) +

  ggnewscale::new_scale_fill() +   # neue Fill-Skala starten

  # Grüne Kachel für Profit
  geom_tile(data = df_profit,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_profit,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Profit",
    low  = "#e5f5e0",
    high = "#238b45"
  ) +

  labs(
    title = "Model Performance Comparison",
    x     = "Metric",
    y     = "Model"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )


```
```{r}
knn_best_k_profit <- data.frame(
  model       = "KNN best k for profit",
  accuracy    = cm_best_k_profit$overall["Accuracy"],
  kappa       = cm_best_k_profit$overall["Kappa"],
  sensitivity = cm_best_k_profit$byClass["Sensitivity"],
  specificity = cm_best_k_profit$byClass["Specificity"],
  profit = gain_function(cm_best_k_profit)/(cm_best_k_profit$table[1,1]+cm_best_k_profit$table[1,2]+cm_best_k_profit$table[2,1]+cm_best_k_profit$table[2,2])
)

# An bestehende summary_table anhängen
summary_table <- rbind(summary_table, knn_best_k_profit)

summary_long <- summary_table %>%
  pivot_longer(
    cols      = -model,
    names_to  = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = factor(metric,
                    levels = c("accuracy", "kappa",
                               "sensitivity", "specificity",
                               "profit")),         # profit ganz rechts
    model  = factor(model, levels = rev(unique(model)))
  )

# 2) Aufteilen in „blaue Metriken“ und „grünen Profit“
df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

# 3) Heatmap mit zwei Farbskalen
ggplot() +
  # Blaue Kacheln für Accuracy/Kappa/Sens/Spec
  geom_tile(data = df_metrics,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_metrics,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Performance",
    low  = "#D8E6F3",
    high = "#084594"
  ) +

  ggnewscale::new_scale_fill() +   # neue Fill-Skala starten

  # Grüne Kachel für Profit
  geom_tile(data = df_profit,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_profit,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Profit",
    low  = "#e5f5e0",
    high = "#238b45"
  ) +

  labs(
    title = "Model Performance Comparison",
    x     = "Metric",
    y     = "Model"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )


```
Interpretation: Across all three KNN specifications, the method performs consistently worse than logistic regression and struggles to model cancellation behavior in a way that would be useful for hotel decision making. The basic KNN model reaches an accuracy of 77.9 percent and a Kappa value of 0.515, indicating only moderate agreement with the true labels. Its sensitivity is just 0.651, meaning it fails to detect about one third of actual cancellations, and this weakness contributes to a low profit of roughly 20.08, which is less than half of what logistic regression delivers. Tuning KNN for maximum Kappa improves sensitivity to 0.722, but specificity falls to 0.818, increasing the number of false positives, which are the most expensive errors for the hotel. This leads to an even lower profit of 17.63 despite the slightly better classification balance. When KNN is tuned for maximum profit, the model increases specificity to 0.906, which reduces overbooking costs, but does so by sacrificing sensitivity, which drops to 0.545. This means it misses more than half of all true cancellations, creating significant opportunity cost. Profit increases to 23.50, making this the best performing KNN variant, but it still falls far short of logistic regression, which achieves both higher predictive accuracy and substantially higher economic value per booking. Taken together, all KNN variants show clear weaknesses relative to logistic regression, either by missing too many cancellations, generating costly false positives, or failing to produce meaningful profit gains.

### ANN

#### Basic ANN

```{r}
model_ann_b <- neuralnet(y ~ ., data = train_first_ann)
plot(model_ann_b)
```

```{r}
train_first_ann$y <- ifelse(train_first_ann$y == 1, 1, 0)

```



```{r}
train_first_dt$y <- factor(train_first_dt$y, labels = c("No", "Yes"))
test_first_dt$y  <- factor(test_first_dt$y,  labels = c("No", "Yes"))
str(train_first_dt)
```

```{r}
summary(data$reserved_room_type)
summary(train_first_log$reserved_room_type)
summary(test_first_log$reserved_room_type)

```

```{r}
table(train_first_dt$y)

```

```{r}
model_ann_b <- neuralnet(y ~ ., data = train_first_ann)
plot(model_ann_b)
```

### SVM

Train the SVM models:
```{r}
for (k in c("rbfdot","polydot","vanilladot"))
  assign(paste0("svm_model_", k), ksvm(y ~ ., data = train_first_svm, kernel = k, prob.model = TRUE))
```


```{r}
thresholds <- seq(0, 1, by = 0.1)

for (k in c("rbfdot","polydot","vanilladot")) {
  for (t in thresholds) {
    assign(paste0("svm_prob_", k), predict(get(paste0("svm_model_", k)), test_first_svm, type = "probabilities"))
    assign(paste0("svm_pred_", k), ifelse(get(paste0("svm_prob_", k))[,2] >= t, 1, 0))
    assign(paste0("svm_cm_", k), confusionMatrix(as.factor(get(paste0("svm_pred_", k))), as.factor(test_first_svm$y), positive = "1"))
    cm <- confusionMatrix(as.factor(get(paste0("svm_pred_", k))), as.factor(test_first_svm$y), positive = "1")
    assign(paste0("svm_gain_", k), gain_function(cm))
    assign(paste0("svm_kappa_", k), as.numeric(cm$overall["Kappa"]))
    assign(paste0("svm_accuracy_", k), as.numeric(cm$overall["Accuracy"]))
    assign(paste0("svm_sensitivity_", k), as.numeric(cm$byClass["Sensitivity"]))
    assign(paste0("svm_specificity_", k), as.numeric(cm$byClass["Specificity"]))
    svm_results <- rbind(svm_results, data.frame(model = k,
                                                   threshold = t,
                                                   accuracy = get(paste0("svm_accuracy_", k)),
                                                   sensitivity = get(paste0("svm_sensitivity_", k)),
                                                   specificity = get(paste0("svm_specificity_", k)),
                                                   kappa = get(paste0("svm_kappa_", k)),
                                                   gain = get(paste0("svm_gain_", k)),
                                                   stringsAsFactors = FALSE, row.names = NULL))
  }
  assign(paste0("svm_standard_", k), head(svm_results[svm_results$model == k & svm_results$threshold == 0.5, ], 1))
  assign(paste0("svm_maxgain_", k), svm_results[svm_results$model == k, ][which.max(svm_results$gain[svm_results$model == k]), ])
  assign(paste0("svm_maxkappa_", k), svm_results[svm_results$model == k, ][which.max(svm_results$kappa[svm_results$model == k]), ])
}

svm_maxgain_results <- svm_results[which.max(svm_results$gain), ]
svm_maxkappa_results <- svm_results[which.max(svm_results$kappa), ]
svm_maxgain <- max(svm_results$gain)
```



```{r}
svm_maxgain_rbfdot_table <- data.frame(
  model       = "SVM rbfdot max profit",
  accuracy    = svm_maxgain_rbfdot["accuracy"],
  kappa       = svm_maxgain_rbfdot["kappa"],
  sensitivity = svm_maxgain_rbfdot["sensitivity"],
  specificity = svm_maxgain_rbfdot["specificity"],
  profit = svm_maxgain_rbfdot["gain"]/nrow(test_first_svm)
)

svm_maxgain_rbfdot_table$profit <- svm_maxgain_rbfdot_table$gain

svm_maxgain_rbfdot_table <- svm_maxgain_rbfdot_table %>%
  select(-gain)


# An bestehende summary_table anhängen
summary_table <- rbind(summary_table, svm_maxgain_rbfdot_table)

summary_long <- summary_table %>%
  pivot_longer(
    cols      = -model,
    names_to  = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = factor(metric,
                    levels = c("accuracy", "kappa",
                               "sensitivity", "specificity",
                               "profit")),         # profit ganz rechts
    model  = factor(model, levels = rev(unique(model)))
  )

# 2) Aufteilen in „blaue Metriken“ und „grünen Profit“
df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

# 3) Heatmap mit zwei Farbskalen
ggplot() +
  # Blaue Kacheln für Accuracy/Kappa/Sens/Spec
  geom_tile(data = df_metrics,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_metrics,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Performance",
    low  = "#D8E6F3",
    high = "#084594"
  ) +

  ggnewscale::new_scale_fill() +   # neue Fill-Skala starten

  # Grüne Kachel für Profit
  geom_tile(data = df_profit,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_profit,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Profit",
    low  = "#e5f5e0",
    high = "#238b45"
  ) +

  labs(
    title = "Model Performance Comparison",
    x     = "Metric",
    y     = "Model"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```


```{r}
svm_maxgain_polydot_table <- data.frame(
  model       = "SVM polydot max profit",
  accuracy    = svm_maxgain_polydot["accuracy"],
  kappa       = svm_maxgain_polydot["kappa"],
  sensitivity = svm_maxgain_polydot["sensitivity"],
  specificity = svm_maxgain_polydot["specificity"],
  profit = svm_maxgain_polydot["gain"]/nrow(test_first_svm)
)

svm_maxgain_polydot_table$profit <- svm_maxgain_polydot_table$gain

svm_maxgain_polydot_table <- svm_maxgain_polydot_table %>%
  select(-gain)


# An bestehende summary_table anhängen
summary_table <- rbind(summary_table, svm_maxgain_polydot_table)

summary_long <- summary_table %>%
  pivot_longer(
    cols      = -model,
    names_to  = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = factor(metric,
                    levels = c("accuracy", "kappa",
                               "sensitivity", "specificity",
                               "profit")),         # profit ganz rechts
    model  = factor(model, levels = rev(unique(model)))
  )

# 2) Aufteilen in „blaue Metriken“ und „grünen Profit“
df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

# 3) Heatmap mit zwei Farbskalen
ggplot() +
  # Blaue Kacheln für Accuracy/Kappa/Sens/Spec
  geom_tile(data = df_metrics,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_metrics,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Performance",
    low  = "#D8E6F3",
    high = "#084594"
  ) +

  ggnewscale::new_scale_fill() +   # neue Fill-Skala starten

  # Grüne Kachel für Profit
  geom_tile(data = df_profit,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_profit,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Profit",
    low  = "#e5f5e0",
    high = "#238b45"
  ) +

  labs(
    title = "Model Performance Comparison",
    x     = "Metric",
    y     = "Model"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```

```{r}
svm_maxgain_vanilladot_table <- data.frame(
  model       = "SVM vanilladot max profit",
  accuracy    = svm_maxgain_vanilladot["accuracy"],
  kappa       = svm_maxgain_vanilladot["kappa"],
  sensitivity = svm_maxgain_vanilladot["sensitivity"],
  specificity = svm_maxgain_vanilladot["specificity"],
  profit = svm_maxgain_vanilladot["gain"]/nrow(test_first_svm)
)

svm_maxgain_vanilladot_table$profit <- svm_maxgain_vanilladot_table$gain

svm_maxgain_vanilladot_table <- svm_maxgain_vanilladot_table %>%
  select(-gain)


# An bestehende summary_table anhängen
summary_table <- rbind(summary_table, svm_maxgain_vanilladot_table)

summary_long <- summary_table %>%
  pivot_longer(
    cols      = -model,
    names_to  = "metric",
    values_to = "value"
  ) %>%
  mutate(
    metric = factor(metric,
                    levels = c("accuracy", "kappa",
                               "sensitivity", "specificity",
                               "profit")),         # profit ganz rechts
    model  = factor(model, levels = rev(unique(model)))
  )

# 2) Aufteilen in „blaue Metriken“ und „grünen Profit“
df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

# 3) Heatmap mit zwei Farbskalen
ggplot() +
  # Blaue Kacheln für Accuracy/Kappa/Sens/Spec
  geom_tile(data = df_metrics,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_metrics,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Performance",
    low  = "#D8E6F3",
    high = "#084594"
  ) +

  ggnewscale::new_scale_fill() +   # neue Fill-Skala starten

  # Grüne Kachel für Profit
  geom_tile(data = df_profit,
            aes(x = metric, y = model, fill = value),
            colour = "white") +
  geom_text(data = df_profit,
            aes(x = metric, y = model, label = round(value, 3)),
            color = "black", size = 4) +
  scale_fill_gradient(
    name = "Profit",
    low  = "#e5f5e0",
    high = "#238b45"
  ) +

  labs(
    title = "Model Performance Comparison",
    x     = "Metric",
    y     = "Model"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  )

```
Interpretation: Across the three SVM variants, performance varies meaningfully depending on the kernel choice. The RBF kernel performs reasonably well, with 85.6% accuracy and strong specificity of 0.974, meaning it is very good at identifying guests who will actually show up. However, its sensitivity is lower at 0.655, so it misses a substantial share of true cancellations. This cautious behavior reduces costly false positives but also limits its financial performance, producing a profit of about 36.13 per booking.

In contrast, the polynomial and linear kernel SVM models perform exceptionally well across every metric. Both achieve around 99% accuracy and Kappa values near 0.98, indicating almost perfect alignment with the true outcomes. Sensitivity and specificity are equally strong for both kernels, showing that they identify cancellations and non-cancellations with near-complete accuracy. Financially, they deliver profits of 49.69 and 49.66, essentially matching logistic regression and far outperforming KNN and the RBF SVM.

Taken together, these results show that the cancellation problem is highly separable using linear and polynomial decision boundaries, while the RBF kernel underfits the structure of the data. The polynomial and linear SVM models stand out as top performers, combining excellent predictive accuracy with strong financial outcomes, making them some of the most reliable and effective models for supporting hotel overbooking decisions.

### Random Forrest
```{r}
#### Basic Random Forest

set.seed(12345)

rf_model_base <- randomForest(
y ~ .,
data   = train_first_rf,
ntree  = 500,
mtry   = floor(sqrt(ncol(train_first_rf) - 1)),
importance = TRUE
)

rf_model_base

```

```{r}
## Predictions on the test set for the base Random Forest

rf_pred_base <- predict(
rf_model_base,
newdata = test_first_rf,
type    = "class"
)

rf_cm_base <- confusionMatrix(
as.factor(rf_pred_base),
as.factor(test_first_rf$y),
positive = "1"
)

rf_cm_base

```


```{r}
## Profit per booking for the base Random Forest

rf_profit_base <- gain_function(rf_cm_base) /
sum(rf_cm_base$table)

rf_profit_base

```

```{r}
## Add base Random Forest to the summary_table and redraw the heatmap

rf_baseline_row <- data.frame(
model       = "Random Forest base",
accuracy    = rf_cm_base$overall["Accuracy"],
kappa       = rf_cm_base$overall["Kappa"],
sensitivity = rf_cm_base$byClass["Sensitivity"],
specificity = rf_cm_base$byClass["Specificity"],
profit      = rf_profit_base
)

summary_table <- rbind(summary_table, rf_baseline_row)

summary_long <- summary_table %>%
pivot_longer(
cols      = -model,
names_to  = "metric",
values_to = "value"
) %>%
mutate(
metric = factor(
metric,
levels = c("accuracy", "kappa",
"sensitivity", "specificity",
"profit")
),
model  = factor(model, levels = rev(unique(model)))
)

df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

ggplot() +
geom_tile(
data  = df_metrics,
aes(x = metric, y = model, fill = value),
colour = "white"
) +
geom_text(
data  = df_metrics,
aes(x = metric, y = model, label = round(value, 3)),
color = "black",
size  = 4
) +
scale_fill_gradient(
name = "Performance",
low  = "#D8E6F3",
high = "#084594"
) +
ggnewscale::new_scale_fill() +
geom_tile(
data  = df_profit,
aes(x = metric, y = model, fill = value),
colour = "white"
) +
geom_text(
data  = df_profit,
aes(x = metric, y = model, label = round(value, 3)),
color = "black",
size  = 4
) +
scale_fill_gradient(
name = "Profit",
low  = "#e5f5e0",
high = "#238b45"
) +
labs(
title = "Model Performance Comparison",
x     = "Metric",
y     = "Model"
) +
theme_minimal(base_size = 14) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
)

```

```{r}
#### Tuned Random Forest

set.seed(12345)

rf_results <- results   # reuse the empty template you defined earlier

## Choose a grid of mtry values to test

p <- ncol(train_first_rf) - 1   # number of predictors
mtry_grid <- unique(round(c(sqrt(p), p / 4, p / 2)))
mtry_grid <- mtry_grid[mtry_grid >= 1]

for (m in mtry_grid) {
rf_tmp <- randomForest(
y ~ .,
data   = train_first_rf,
ntree  = 500,
mtry   = m
)

rf_pred_tmp <- predict(
rf_tmp,
newdata = test_first_rf,
type    = "class"
)

cm <- confusionMatrix(
as.factor(rf_pred_tmp),
as.factor(test_first_rf$y),
positive = "1"
)

rf_gain_tmp        <- gain_function(cm)
rf_kappa_tmp       <- as.numeric(cm$overall["Kappa"])
rf_accuracy_tmp    <- as.numeric(cm$overall["Accuracy"])
rf_sensitivity_tmp <- as.numeric(cm$byClass["Sensitivity"])
rf_specificity_tmp <- as.numeric(cm$byClass["Specificity"])

rf_results <- rbind(
rf_results,
data.frame(
model       = "rf",
threshold   = m,      # here threshold stores mtry
kappa       = rf_kappa_tmp,
gain        = rf_gain_tmp,
accuracy    = rf_accuracy_tmp,
sensitivity = rf_sensitivity_tmp,
specificity = rf_specificity_tmp,
stringsAsFactors = FALSE,
row.names       = NULL
)
)
}

rf_maxgain_results <- rf_results[which.max(rf_results$gain), ]
rf_maxkappa_results <- rf_results[which.max(rf_results$kappa), ]

rf_maxgain_results
rf_maxkappa_results

```


```{r}
## Best Random Forest by Kappa

best_mtry_kappa <- rf_maxkappa_results$threshold

set.seed(12345)
rf_best_kappa <- randomForest(
y ~ .,
data   = train_first_rf,
ntree  = 500,
mtry   = best_mtry_kappa
)

rf_pred_best_kappa <- predict(
rf_best_kappa,
newdata = test_first_rf,
type    = "class"
)

cm_rf_best_kappa <- confusionMatrix(
as.factor(rf_pred_best_kappa),
as.factor(test_first_rf$y),
positive = "1"
)

cm_rf_best_kappa

rf_profit_best_kappa <- gain_function(cm_rf_best_kappa) /
sum(cm_rf_best_kappa$table)

rf_row_best_kappa <- data.frame(
model       = "Random Forest best mtry for kappa",
accuracy    = cm_rf_best_kappa$overall["Accuracy"],
kappa       = cm_rf_best_kappa$overall["Kappa"],
sensitivity = cm_rf_best_kappa$byClass["Sensitivity"],
specificity = cm_rf_best_kappa$byClass["Specificity"],
profit      = rf_profit_best_kappa
)

summary_table <- rbind(summary_table, rf_row_best_kappa)

```

```{r}
## Best Random Forest by profit

best_mtry_profit <- rf_maxgain_results$threshold

set.seed(12345)
rf_best_profit <- randomForest(
y ~ .,
data   = train_first_rf,
ntree  = 500,
mtry   = best_mtry_profit
)

rf_pred_best_profit <- predict(
rf_best_profit,
newdata = test_first_rf,
type    = "class"
)

cm_rf_best_profit <- confusionMatrix(
as.factor(rf_pred_best_profit),
as.factor(test_first_rf$y),
positive = "1"
)

cm_rf_best_profit

rf_profit_best_profit <- gain_function(cm_rf_best_profit) /
sum(cm_rf_best_profit$table)

rf_row_best_profit <- data.frame(
model       = "Random Forest best mtry for profit",
accuracy    = cm_rf_best_profit$overall["Accuracy"],
kappa       = cm_rf_best_profit$overall["Kappa"],
sensitivity = cm_rf_best_profit$byClass["Sensitivity"],
specificity = cm_rf_best_profit$byClass["Specificity"],
profit      = rf_profit_best_profit
)

summary_table <- rbind(summary_table, rf_row_best_profit)

```

```{r}
summary_long <- summary_table %>%
pivot_longer(
cols      = -model,
names_to  = "metric",
values_to = "value"
) %>%
mutate(
metric = factor(
metric,
levels = c("accuracy", "kappa",
"sensitivity", "specificity",
"profit")
),
model  = factor(model, levels = rev(unique(model)))
)

df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

ggplot() +
geom_tile(
data  = df_metrics,
aes(x = metric, y = model, fill = value),
colour = "white"
) +
geom_text(
data  = df_metrics,
aes(x = metric, y = model, label = round(value, 3)),
color = "black",
size  = 4
) +
scale_fill_gradient(
name = "Performance",
low  = "#D8E6F3",
high = "#084594"
) +
ggnewscale::new_scale_fill() +
geom_tile(
data  = df_profit,
aes(x = metric, y = model, fill = value),
colour = "white"
) +
geom_text(
data  = df_profit,
aes(x = metric, y = model, label = round(value, 3)),
color = "black",
size  = 4
) +
scale_fill_gradient(
name = "Profit",
low  = "#e5f5e0",
high = "#238b45"
) +
labs(
title = "Model Performance Comparison",
x     = "Metric",
y     = "Model"
) +
theme_minimal(base_size = 14) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
)

```
Interpretation: The Random Forest models perform strongly overall, and each version of the model shows competitive predictive accuracy and financial performance. The base Random Forest achieves 93.5% accuracy with solid sensitivity (0.874) and strong specificity (0.971). This means it correctly identifies most cancellations and almost all non-cancellations, making it safer for overbooking decisions than KNN but slightly less precise than the top SVM and logistic models. Its profit of 42.88 per booking reflects this balance: high overall accuracy but occasional costly misclassifications.

Tuning the model by adjusting the mtry parameter improves performance noticeably. Both the best mtry for Kappa and the best mtry for profit converge to nearly identical performance: 95.4% accuracy, Kappa of 0.9, and sensitivity of 0.906. The improvement in sensitivity is especially valuable because correctly identifying cancellations reduces empty-room losses. Specificity also remains extremely high at 0.982, which prevents false positives that would lead to expensive walk situations. These tuned models produce a profit of about 45.46, a meaningful improvement over the baseline Random Forest and substantially better than all KNN variants.

Overall, Random Forest offers a strong balance of predictive power and financial value, especially after tuning. While it does not quite reach the near-perfect performance of logistic regression or the top SVM models, the tuned Random Forest still ranks among the most reliable models for supporting hotel overbooking decisions. Its robustness to noise and ability to capture nonlinear patterns make it a dependable choice in this setting.

### Decision Tree

```{r}
### Decision Tree (DT)

# Quick sanity check (read only, no modification)

str(train_first_dt$y)
table(train_first_dt$y)

# Basic decision tree with rpart

dt_model_base <- rpart(
y ~ .,
data   = train_first_dt,
method = "class",
control = rpart.control(
cp        = 0.01,   # pruning strength
minbucket = 50      # minimum obs in terminal node
)
)

# Visualize the tree structure

rpart.plot(
dt_model_base,
type  = 2,
extra = 104,
tweak = 1.2,
main  = "Decision Tree for Hotel Booking Cancellations"
)

# Predictions on the test set

dt_pred_base <- predict(
dt_model_base,
newdata = test_first_dt,
type    = "class"
)

# Confusion matrix for the base tree

dt_conf_base <- confusionMatrix(
as.factor(dt_pred_base),
as.factor(test_first_dt$y),
positive = "1"
)

dt_conf_base

```

```{r}
# Add DT row to summary_table

dt_baseline_row <- data.frame(
model       = "Decision Tree",
accuracy    = dt_conf_base$overall["Accuracy"],
kappa       = dt_conf_base$overall["Kappa"],
sensitivity = dt_conf_base$byClass["Sensitivity"],
specificity = dt_conf_base$byClass["Specificity"],
profit      = gain_function(dt_conf_base) / sum(dt_conf_base$table),
stringsAsFactors = FALSE
)

summary_table <- rbind(summary_table, dt_baseline_row)

# Long format for plotting

summary_long <- summary_table %>%
pivot_longer(
cols      = -model,
names_to  = "metric",
values_to = "value"
) %>%
mutate(
metric = factor(
metric,
levels = c("accuracy", "kappa",
"sensitivity", "specificity",
"profit")
),
model  = factor(model, levels = rev(unique(model)))
)

df_metrics <- dplyr::filter(summary_long, metric != "profit")
df_profit  <- dplyr::filter(summary_long, metric == "profit")

# Heatmap with separate scales for performance and profit

ggplot() +
geom_tile(
data  = df_metrics,
aes(x = metric, y = model, fill = value),
colour = "white"
) +
geom_text(
data  = df_metrics,
aes(x = metric, y = model, label = round(value, 3)),
color = "black",
size  = 4
) +
scale_fill_gradient(
name = "Performance",
low  = "#D8E6F3",
high = "#084594"
) +
ggnewscale::new_scale_fill() +
geom_tile(
data  = df_profit,
aes(x = metric, y = model, fill = value),
colour = "white"
) +
geom_text(
data  = df_profit,
aes(x = metric, y = model, label = round(value, 3)),
color = "black",
size  = 4
) +
scale_fill_gradient(
name = "Profit",
low  = "#e5f5e0",
high = "#238b45"
) +
labs(
title = "Model Performance Comparison",
x     = "Metric",
y     = "Model"
) +
theme_minimal(base_size = 14) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1)
)

```

Interpretation: The decision tree model performs moderately well overall but clearly trails behind the more advanced models in both predictive accuracy and economic value. It reaches an accuracy of 82.6% with a Kappa of 0.618, indicating weaker agreement with the true cancellation labels compared to logistic regression, SVMs, and Random Forests. Sensitivity is 0.706, meaning the model correctly identifies about seventy percent of all cancellations while missing a sizeable portion. Specificity is stronger at 0.897, so it is more reliable at detecting guests who actually show up.

This imbalance has direct financial implications. With an expected profit of about 27.62 per booking, the decision tree performs better than the KNN methods but falls far short of the SVMs, logistic regression, and the tuned Random Forests. The lower sensitivity increases false negatives, leading to more unsold rooms, while occasional false positives remain costly due to walk penalties.

Overall, while the decision tree provides a simple and interpretable model, its reduced predictive precision makes it less effective for decisions involving asymmetric costs like hotel overbooking. It may be useful for understanding broad patterns in the data, but it is not competitive with the other models when financial performance is the priority.

